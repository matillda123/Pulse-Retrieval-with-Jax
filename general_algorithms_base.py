import numpy as np

import jax.numpy as jnp
import jax
from jax.tree_util import Partial
from equinox import tree_at

import equinox
import optimistix

from utilities import scan_helper, get_com, optimistix_helper_loss_function, scan_helper_equinox, MyNamespace, do_fft, do_ifft, calculate_trace, loss_function_modifications, do_interpolation_1d
from BaseClasses import AlgorithmsBASE
from create_population import create_population_general
from bsplines_1d import make_bsplines, get_prefactor, get_M









class GeneralOptimizationBASE(AlgorithmsBASE):
    """
    The Base-Class for all general solvers. Inherits from AlgorithmsBASE.

    Attributes:
        use_fd_grad: bool or int,
        amplitude_or_intensity: str,
        error_metric: Callable,
        bspline_info: Pytree,
    
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.use_fd_grad = False
        self.amplitude_or_intensity = "intensity"
        self.error_metric = self.trace_error

        self.bspline_info = MyNamespace(amp = None, phase = None)


    def create_initial_population(self, population_size, amp_type="gaussian", phase_type="polynomial", no_funcs_amp=5, no_funcs_phase=6):
        """ 
        Creates an initial guess either explicit or parametrized. 

        Args:
            population_size: int, the number of individuals
            amp_type: str, the representation of the spectral amplitude, can be one of gaussian, lorentzian, bsplines or discrete
            phase_type: str, the representation of the spectral phase, can be one of polynomial, sinusoidal, sigmoidal, bsplines or discrete
            no_funcs_amp: int, the number of basis functions for the spectral amplitude (if parametrized)
            no_funcs_phase: int, the number of basis functions for the spectral phase (if parametrized)

        Returns:
            Pytree, the initial guess population
        
        """

        self.key, subkey = jax.random.split(self.key, 2)

        population=MyNamespace(pulse=MyNamespace(amp=None, phase=None), 
                               gate=MyNamespace(amp=None, phase=None))
    
        if phase_type[:-2]=="bsplines":
            k, phase_type = int(phase_type[-1]), phase_type[:-2]
            f, M = get_prefactor(k), get_M(k)

            # spline order and number of f-points restrict valid number of control-points
            N = jnp.size(self.measurement_info.frequency)
            n = no_funcs_phase
            nn = jnp.divide(N, jnp.linspace(1, jnp.ceil(N/n), int(jnp.ceil(N/n))))
            no_funcs_phase = int(nn[jnp.round(nn%1, 5)==0][-1]) + (k-1)

            Nx = N/(no_funcs_phase-k+1) + 1 # how many points per spline?
            bspline_info_phase = MyNamespace(k=k, M=M, f=f, Nx=int(Nx))
            self.bspline_info = self.bspline_info.expand(phase = bspline_info_phase)

        if amp_type[:-2]=="bsplines":
            k, amp_type = int(amp_type[-1]), amp_type[:-2]
            f, M = get_prefactor(k), get_M(k)
            
            N = jnp.size(self.measurement_info.frequency)
            n = no_funcs_amp
            nn = jnp.divide(N, jnp.linspace(1, jnp.ceil(N/n), int(jnp.ceil(N/n))))
            no_funcs_amp = int(nn[jnp.round(nn%1, 5)==0][-1]) + (k-1)

            Nx = N/(no_funcs_amp-k+1) + 1
            bspline_info_amp = MyNamespace(k=k, M=M, f=f, Nx=int(Nx))
            self.bspline_info = self.bspline_info.expand(amp = bspline_info_amp)

        subkey, population_pulse = create_population_general(subkey, amp_type, phase_type, population.pulse, population_size, no_funcs_amp, no_funcs_phase, 
                                                             self.descent_info.measured_spectrum_is_provided.pulse, self.measurement_info)
        population = tree_at(lambda x: x.pulse, population, population_pulse)
        
        if self.doubleblind==True:
            subkey, population_gate = create_population_general(subkey, amp_type, phase_type, population.gate, population_size, no_funcs_amp, no_funcs_phase, 
                                                                self.descent_info.measured_spectrum_is_provided.gate, self.measurement_info)
            population = tree_at(lambda x: x.gate, population, population_gate, is_leaf=lambda x: x is None)
            

        classical_guess_types=["random", "random_phase", "constant", "constant_phase"]
        if any([guess==phase_type for guess in classical_guess_types])==True:
            phase_type = "discrete"
            
        if any([guess==amp_type for guess in classical_guess_types])==True:
            amp_type = "discrete"
        
        self.descent_info = self.descent_info.expand(population_size=population_size, phase_type=phase_type, amp_type=amp_type)
        return population
    


    def split_population_in_amp_and_phase(self, population):
        """ Splits a population into an amplitude and phase population. """
        population_amp = MyNamespace(pulse=MyNamespace(amp=population.pulse.amp, phase=None), 
                                     gate=MyNamespace(amp=population.gate.amp, phase=None))
        
        population_phase = MyNamespace(pulse=MyNamespace(amp=None, phase=population.pulse.phase), 
                                       gate=MyNamespace(amp=None, phase=population.gate.phase))

        return population_amp, population_phase
    

    def merge_population_from_amp_and_phase(self, population_amp, population_phase):
        """ Undoes split_population_in_amp_and_phase() """
        population = MyNamespace(pulse=MyNamespace(amp=population_amp.pulse.amp, phase=population_phase.pulse.phase), 
                                 gate=MyNamespace(amp=population_amp.gate.amp, phase=population_phase.gate.phase))
        return population




    def polynomial_term(self, coefficient, order, x0, x):
        return coefficient*(x-x0)**order
    

    def polynomial_phase(self, coefficients, central_f, measurement_info, descent_info):
        phase = jax.vmap(self.polynomial_term, in_axes=(0, 0, None, None))(coefficients, jnp.arange(jnp.size(coefficients))+1, central_f, measurement_info.frequency)
        phase = jnp.sum(phase, axis=0)
        return phase
    

    def sinusoidal_term(self, a, b, c, x):
        phase = a*jnp.sin(2*jnp.pi*b*x+c)
        return phase
    
    def sinusoidal_phase(self, coefficients, central_f, measurement_info, descent_info):
        a, b, c = coefficients.a, coefficients.b, coefficients.c
        phase_arr = jax.vmap(self.sinusoidal_term, in_axes=(0, 0, 0, None))(a, b, c, measurement_info.frequency)
        phase = jnp.sum(phase_arr, axis=0)
        return phase
    

    def discrete_phase(self, coefficients, central_f, measurement_info, descent_info):
        return coefficients
    

    def tanh_term(self, c, k, x):
        return 0.5*(1+jnp.tanh((x-c)/k))
    

    def tanh_phase(self, coefficients, central_f, measurement_info, descent_info):
        a, c, k = coefficients.a, coefficients.c, coefficients.k
        phase_arr=jax.vmap(self.tanh_term, in_axes=(0, 0, None))(c, k, measurement_info.frequency)
        phase = jnp.sum(a[:, jnp.newaxis]*phase_arr, axis=0)
        return phase
    

    def bspline_phase(self, coefficients, central_f, measurement_info, descent_info):
        bspline_info = descent_info.bsplines.phase
        k, M, f, Nx = bspline_info.k, bspline_info.M, bspline_info.f, bspline_info.Nx
        phase = make_bsplines(coefficients.c, k, M, f, Nx)
        return phase



    def gaussian_term(self, a, b, c, frequency):
        b = b/2.355 # go from fwhm to sigma
        return a*jnp.exp(-(frequency-c)**2/(2*b**2+1e-9))
    

    def gaussian_amplitude(self, coefficients, measurement_info, descent_info):
        a, b, c = coefficients.a, coefficients.b, coefficients.c
        amp_f = jax.vmap(self.gaussian_term, in_axes=(0, 0, 0, None))(a, b, c, measurement_info.frequency)
        amp_f = jnp.sum(amp_f, axis=0)
        return amp_f



    
    def lorentzian_term(self, a, b, c, frequency):
        b = b/2 # go from fwhm to sigma
        return a/((frequency-c)**2+b**2)
    

    def lorentzian_amplitude(self, coefficients, measurement_info, descent_info):
        a, b, c = coefficients.a, coefficients.b, coefficients.c
        amp_f = jax.vmap(self.lorentzian_term, in_axes=(0, 0, 0, None))(a, b, c, measurement_info.frequency)
        amp_f = jnp.sum(amp_f, axis=0)
        return amp_f
    

    

    def discrete_amplitude(self, coefficients, measurement_info, descent_info):
        amp_f = coefficients
        return amp_f
    

    def bspline_amplitude(self, coefficients, measurement_info, descent_info):
        bspline_info = descent_info.bsplines.amp
        k, M, f, Nx = bspline_info.k, bspline_info.M, bspline_info.f, bspline_info.Nx
        amp_f = make_bsplines(coefficients.c, k, M, f, Nx)
        return amp_f
    



    def get_phase(self, coefficients, central_f, measurement_info, descent_info):
        """ Evaluates the spectral phase onto the frequency axis. """
        spectral_phase_func_dict={"polynomial": self.polynomial_phase,
                                  "sinusoidal": self.sinusoidal_phase,
                                  "sigmoidal": self.tanh_phase,
                                  "bsplines": self.bspline_phase,
                                  "discrete": self.discrete_phase}
        
        spectral_phase_func = spectral_phase_func_dict[descent_info.phase_type]
        return spectral_phase_func(coefficients, central_f, measurement_info, descent_info)
    


    def get_amplitude(self, coefficients, measurement_info, descent_info):
        """ Evaluates the spectral amplitude onto the frequency axis. """
        amp_func_dict={"gaussian": self.gaussian_amplitude,
                       "lorentzian": self.lorentzian_amplitude,
                       "bsplines": self.bspline_amplitude,
                       "discrete": self.discrete_amplitude}
            
        amp_func = amp_func_dict[descent_info.amp_type]
        amp_f = amp_func(coefficients, measurement_info, descent_info)

        frequency = measurement_info.frequency
        idx_arr = jnp.arange(jnp.size(frequency))
        idx = jnp.sum(idx_arr*jnp.abs(amp_f))/jnp.sum(jnp.abs(amp_f))
        central_f = frequency[idx.astype(int)]
        return amp_f, central_f
    



    def make_pulse_f_from_individual(self, individual, measurement_info, descent_info, pulse_or_gate="pulse"):
        """ Evaluates a parametrized individual onto the frequency axis. """
        individual = getattr(individual, pulse_or_gate)

        if getattr(descent_info.measured_spectrum_is_provided, pulse_or_gate)==True:
            amp = getattr(measurement_info.spectral_amplitude, pulse_or_gate)
            central_f = getattr(measurement_info.central_f, pulse_or_gate)

        else:
            amp, central_f = self.get_amplitude(individual.amp, measurement_info, descent_info)
    
        phase = self.get_phase(individual.phase, central_f, measurement_info, descent_info)
        signal_f = amp*jnp.exp(1j*2*jnp.pi*phase)
        return signal_f
    


    def get_pulses_f_from_population(self, population, measurement_info, descent_info):
        """ Evaluates a parametrized population onto the frequency axis. """
        make_pulse = Partial(self.make_pulse_f_from_individual, measurement_info=measurement_info, descent_info=descent_info, pulse_or_gate="pulse")
        pulse_f_arr = jax.vmap(make_pulse)(population)

        if measurement_info.doubleblind==True:
            make_gate = Partial(self.make_pulse_f_from_individual, measurement_info=measurement_info, descent_info=descent_info, pulse_or_gate="gate")
            gate_arr = jax.vmap(make_gate)(population)
        else:
            gate_arr = pulse_f_arr

        return pulse_f_arr, gate_arr
    
    

    def make_pulse_t_from_individual(self, individual, measurement_info, descent_info, pulse_or_gate="pulse"):
        """ Evaluates a parametrized individual onto the time axis. """
        signal_f = self.make_pulse_f_from_individual(individual, measurement_info, descent_info, pulse_or_gate)
        signal = do_ifft(signal_f, measurement_info.sk, measurement_info.rn)
        return signal
    
    

    def get_pulses_t_from_population(self, population, measurement_info, descent_info):
        """ Evaluates a parametrized population onto the time axis. """
        pulse_f_arr, gate_arr = self.get_pulses_f_from_population(population, measurement_info, descent_info)
        pulse_t_arr = do_ifft(pulse_f_arr, measurement_info.sk, measurement_info.rn)

        if measurement_info.doubleblind==True:
            gate_arr = do_ifft(gate_arr, measurement_info.sk, measurement_info.rn)
        else:
            gate_arr = pulse_t_arr

        return pulse_t_arr, gate_arr
    



    def construct_trace(self, individual, measurement_info, descent_info):
        """ Generates a trace for a given individual. Calls the method specific function for calculating the nonlinear signal fields. """
        x_arr, frequency = measurement_info.x_arr, measurement_info.frequency
        sk, rn = measurement_info.sk, measurement_info.rn
        
        signal_t = self.calculate_signal_t(individual, measurement_info.transform_arr, measurement_info)
        signal_f = do_fft(signal_t.signal_t, sk, rn)
        trace = calculate_trace(signal_f)
        return x_arr, frequency, trace
    

    def trace_error(self, trace, measured_trace):
        """ The mean least squares error. """
        return jnp.mean(jnp.abs(trace - measured_trace)**2)

    def calculate_error_individual(self, individual, measurement_info, descent_info):
        """ Calculates the error of an individual based on its trace. 
        Allows modification of the error-function via error_metric() and loss_function_modification(). """
        measured_trace = measurement_info.measured_trace
        amplitude_or_intensity, use_fd_grad = descent_info.amplitude_or_intensity, descent_info.use_fd_grad
        error_metric = descent_info.error_metric

        x_arr, y_arr, trace = self.construct_trace(individual, measurement_info, descent_info)
        trace, measured_trace = loss_function_modifications(trace, measured_trace, x_arr, y_arr, amplitude_or_intensity, use_fd_grad)
        trace_error = error_metric(trace, measured_trace)
        return trace_error
    


    def calculate_error_population(self, population, measurement_info, descent_info):
        """ Calls jax.vmap over calculate_error_individual() for an entire population. """
        pulse_arr, gate_arr = self.get_pulses_from_population(population, measurement_info, descent_info)
        error_arr = jax.vmap(self.calculate_error_individual, in_axes=(0, None, None))(MyNamespace(pulse=pulse_arr, gate=gate_arr), measurement_info, descent_info)
        return error_arr
    




    def initialize_general_optimizer(self, population):
        """ A common initialization step for all general solvers. """
        if self.descent_info.measured_spectrum_is_provided.pulse==True:
            spectrum = self.measurement_info.spectral_amplitude.pulse
            idx = get_com(spectrum, jnp.arange(jnp.size(spectrum)))
            self.measurement_info = tree_at(lambda x: x.central_f.pulse, self.measurement_info, self.frequency[int(idx)], is_leaf=lambda x: x is None)

        if self.descent_info.measured_spectrum_is_provided.gate==True:
            spectrum = self.measurement_info.spectral_amplitude.gate
            idx = get_com(spectrum, jnp.arange(jnp.size(spectrum)))
            self.measurement_info = tree_at(lambda x: x.central_f.gate, self.measurement_info, self.frequency[int(idx)], is_leaf=lambda x: x is None)


        self.descent_info = self.descent_info.expand(use_fd_grad = self.use_fd_grad,
                                                     amplitude_or_intensity = self.amplitude_or_intensity,
                                                     error_metric = self.error_metric,
                                                     bsplines = self.bspline_info)

        self.descent_state = self.descent_state.expand(population = population)




    def get_idx_best_individual(self, descent_state):
        """ Calculates the error for a population. Returns the index of the lowest individual. """
        population = descent_state.population
        pulse_arr, gate_arr = self.get_pulses_from_population(population, self.measurement_info, self.descent_info)
        error_arr = jax.vmap(self.calculate_error_individual, in_axes=(0, None, None))(MyNamespace(pulse=pulse_arr, gate=gate_arr), 
                                                                                       self.measurement_info, self.descent_info)
        idx = jnp.argmin(error_arr)
        return idx



    def post_process_get_pulse_and_gate(self, descent_state, measurement_info, descent_info):
        """ Post-processing to evaluate parametrized individuals. """
        idx = self.get_idx_best_individual(descent_state)
        individual = self.get_individual_from_idx(idx, descent_state.population)

        pulse_t = self.make_pulse_t_from_individual(individual, measurement_info, descent_info, "pulse")
        pulse_f = self.make_pulse_f_from_individual(individual, measurement_info, descent_info, "pulse")

        if measurement_info.doubleblind==True:
            gate_t = self.make_pulse_t_from_individual(individual, measurement_info, descent_info, "gate")
            gate_f = self.make_pulse_f_from_individual(individual, measurement_info, descent_info, "gate")
        else:
            gate_t, gate_f = pulse_t, pulse_f

        return pulse_t, gate_t, pulse_f, gate_f
    










def make_key_tree(key, pytree):
    """ For a given pytree, each leaf is replaced by a prng-key. """
    leaves, treedef = jax.tree.flatten(pytree)
    keys = jax.random.split(key, len(leaves))
    keys_tree = jax.tree.unflatten(treedef, keys)
    return keys_tree

def shuffle_pytree(key, pytree, axis):
    """ Randomizes the leafs of a given pytree along a given axis. """
    keys_tree = make_key_tree(key, pytree)
    pytree = jax.tree.map(Partial(jax.random.permutation, axis=axis), keys_tree, pytree)
    return pytree


class DifferentialEvolutionBASE(GeneralOptimizationBASE):
    """
    Implements a Differential-Evolution Algorithm. Inherits from GeneralOptimizationBASE.
    Based on Qiang, J., Mitchell, C., A Unified Differential Evolution Algorithm for Global Optimization, 2014, https://www.osti.gov/servlets/purl/1163659

    Attributes:
        strategy: str,
        mutation_rate: float,
        crossover_rate: float,
        selection_mechanism: str,
        temperature: float,

    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.name = "DifferentialEvolution"

        self.strategy = "best1_bin"
        self.mutation_rate = 0.5
        self.crossover_rate = 0.7
        self.selection_mechanism = "greedy"
        self.temperature = 0.1


        
    def best1(self, F, best_individual, population, key):
        keys = jax.random.split(key, 2)
        population1, population2 = [shuffle_pytree(key, population, 0) for key in keys]
        
        population = best_individual + F*(population1 + (-1)*population2)
        return population

    def best2(self, F, best_individual, population, key):
        keys = jax.random.split(key, 4)
        population1, population2, population3, population4 = [shuffle_pytree(key, population, 0) for key in keys]
        
        population = best_individual + F*(population1 + (-1)*population2 + population3 + (-1)*population4)
        return population

    def rand1(self, F, best_individual, population, key):
        keys = jax.random.split(key, 3)
        population1, population2, population3 = [shuffle_pytree(key, population, 0) for key in keys]

        population = population1 + F*(population2 + (-1)*population3)
        return population

    def rand2(self, F, best_individual, population, key):
        keys = jax.random.split(key, 5)
        population1, population2, population3, population4, population5 = [shuffle_pytree(key, population, 0) for key in keys]

        population = population1 + F*(population2 + (-1)*population3 + population4 + (-1)*population5)
        return population

    def randtobest1(self, F, best_individual, population, key):
        keys = jax.random.split(key, 3)
        population1, population2, population3 = [shuffle_pytree(key, population, 0) for key in keys]

        population = population1 + F*(best_individual + (-1)*population + population2 + (-1)*population3)
        return population

    def randtobest2(self, F, best_individual, population, key):
        keys = jax.random.split(key, 5)
        population1, population2, population3, population4, population5 = [shuffle_pytree(key, population, 0) for key in keys]

        population = population1 + F*(best_individual + (-1)*population + population2 + (-1)*population3 + population4 + (-1)*population5)
        return population

    def currenttorand1(self, F, best_individual, population, key):
        keys = jax.random.split(key, 3)
        population1, population2, population3 = [shuffle_pytree(key, population, 0) for key in keys]

        population = population + F*(population1 + (-1)*population + population2 + (-1)*population3)
        return population

    def currenttorand2(self, F, best_individual, population, key):
        keys = jax.random.split(key, 5)
        population1, population2, population3, population4, population5 = [shuffle_pytree(key, population, 0) for key in keys]

        population = population + F*(population1 + (-1)*population + population2 + (-1)*population3 + population4 + (-1)*population5)
        return population

    def currenttobest1(self, F, best_individual, population, key):
        keys = jax.random.split(key, 2)
        population1, population2 = [shuffle_pytree(key, population, 0) for key in keys]

        population = population + F*(best_individual + (-1)*population + population1 + (-1)*population2)
        return population

    def currenttobest2(self, F, best_individual, population, key):
        keys = jax.random.split(key, 4)
        population1, population2, population3, population4 = [shuffle_pytree(key, population, 0) for key in keys]

        population = population + F*(best_individual + (-1)*population + population1 + (-1)*population2 + population3 + (-1)*population4)
        return population


    

    def make_bin_mask_tree(self, key, pytree, p):
        """ For a given pytree a random binary mask is generated on each leaf, via the probabilty p. """
        leaves, treedef = jax.tree.flatten(pytree)
        N = len(leaves)
        keys = jax.random.split(key, N)
        masks = [jax.random.choice(keys[i], jnp.array([0, 1]), jnp.shape(leaves[i]), p=jnp.array([1-p, p])) for i in range(N)]
        masks_tree = jax.tree.unflatten(treedef, masks)
        return masks_tree


    def make_exp_mask_tree(self, key, pytree, p):
        """ For a given pytree a mask (e.g. of the form 111100000) is generated via the probability p on each row of each leaf. """
        leaves, treedef = jax.tree.flatten(pytree)
        N = len(leaves)
        keys = jax.random.split(key, N)
        masks = [jax.random.choice(keys[i], 
                                   jnp.tri(jnp.shape(leaves[i])[1]) , 
                                   (jnp.shape(leaves[i])[0],), 
                                   p=p**jnp.arange(jnp.shape(leaves[i])[1])*(1-p)
                                   ) for i in range(N)]
        masks_tree = jax.tree.unflatten(treedef, masks)
        return masks_tree


    def bin_crossover(self, CR, parent_population, mutant_population, key):
        """ Peforms a binary crossover between two populations via the crossover rate CR. """
        masks = self.make_bin_mask_tree(key, parent_population, CR)
        population = mutant_population*masks + parent_population*(1 + (-1)*masks)
        return population


    def exp_crossover(self, CR, parent_population, mutant_population, key):
        """ Peforms an exponential crossover between two populations via the crossover rate CR. """
        masks = self.make_exp_mask_tree(key, parent_population, CR)
        population = mutant_population*masks + parent_population*(1 + (-1)*masks)
        return population



    def smooth_crossover(self, CR, parent_population, mutant_population, key):
        """ Similar to an exponential crossover but with a smooth nonbinary transition, which is fascilitated via a tanh function. """
        leaves, treedef = jax.tree.flatten(parent_population)
        N = len(leaves)

        key1, key2 = jax.random.split(key, 2)
        keys1 = jax.random.split(key1, N)
        keys2 = jax.random.split(key2, N)
        
        k_vals = [jax.random.uniform(keys1[i], (jnp.shape(leaves[i])[0], 1), minval=-2, maxval=0) for i in range(N)]
        c_vals = [jax.random.choice(keys2[i], 
                                    jnp.arange(jnp.shape(leaves[i])[1]), 
                                    (jnp.shape(leaves[i])[0], 1), 
                                    p = 1/(jnp.sqrt(2*jnp.pi*CR**2))*jnp.exp(-1/(2*CR**2)*(jnp.arange(jnp.shape(leaves[i])[1])-jnp.shape(leaves[i])[1]//2)**2)
                                    ) for i in range(N)]
        S_vals = [self.tanh_term(c_vals[i], k_vals[i], jnp.arange(jnp.shape(leaves[i])[1])) for i in range(N)]
        S_tree = jax.tree.unflatten(treedef, S_vals)
        population = mutant_population*S_tree + parent_population*(1 + (-1)*S_tree)
        return population
    


    def custom_mutation(self, F, best_individual, population, key):
        """ A placeholder to allow the introduction of custom mutation approaches. """
        pass

    def custom_crossover(self, CR, parent_population, mutant_population, key):
        """ A placeholder to allow the introduction of custom crossover approaches. """
        pass
    
    def do_mutation(self, mutation_strategy, F, best_individual, population, key):
        """ Creates and applies random mutations to a population. """
        mutation_func_dict={"best1": self.best1,
                            "best2": self.best2,
                            "rand1": self.rand1,
                            "rand2": self.rand2,
                            "randtobest1": self.randtobest1,
                            "randtobest2": self.randtobest2,
                            "currenttorand1": self.currenttorand1,
                            "currenttorand2": self.currenttorand2,
                            "currenttobest1": self.currenttobest1,
                            "currenttobest2": self.currenttobest2,
                            "custom": self.custom_mutation}
        
        population = mutation_func_dict[mutation_strategy](F, best_individual, population, key)
        return population


    def do_crossover(self, crossover_strategy, CR, parent_population, mutant_population, key):
        """ Creates and applies random crossover events between two populations. """
        crossover_func_dict={"bin": self.bin_crossover,
                            "exp": self.exp_crossover,
                            "smooth": self.smooth_crossover,
                            "custom": self.custom_crossover}
        
        population = crossover_func_dict[crossover_strategy](CR, parent_population, mutant_population, key)
        return population



    def select_population(self, key, selection_mechanism, error_parent, error_trial, population_parent, population_trial, descent_info):
        """
        Performs the evolutionary selection process for two populations. Currently selection_mechanism can be greedy or global.

        greedy: implements a pairwise comparison between individuals of the two population, where always the individual with the higher fitness is selected.
        global: implements a comparison/ranking between all individuals of both populations. The next generation is selected via randomized sampling 
                based on a Fermi-Distribution. A "temperature" allows tuning of this selection process. 
        
        """

        if selection_mechanism=="greedy":
            trial_smaller = (jnp.sign(error_parent - error_trial)+1)//2
            leaves, treedef = jax.tree.flatten(population_parent)
            trial_smaller_leaves = [trial_smaller[:, jnp.newaxis] for _ in range(len(leaves))]
            trial_smaller_tree = jax.tree.unflatten(treedef, trial_smaller_leaves)

            error = error_trial*trial_smaller + error_parent*(1 + (-1)*trial_smaller)
            population = population_trial*trial_smaller_tree + population_parent*(1 + (-1)*trial_smaller_tree)

        elif selection_mechanism=="global":
            N, temperature = descent_info.population_size, descent_info.temperature

            leaves_parent, treedef = jax.tree.flatten(population_parent)
            leaves_trial, treedef = jax.tree.flatten(population_trial)
            leaves_merged = [jnp.vstack((leaves_parent[i], leaves_trial[i])) for i in range(len(leaves_parent))]  # this can maybe be done with tree.map?

            error_merged = jnp.hstack((error_parent, error_trial))
            idx = jnp.argsort(error_merged)
            p_arr = 1/(jnp.exp((jnp.arange(jnp.size(idx))-N)/(temperature+1e-12))+1)
            p_arr = p_arr/jnp.sum(p_arr)
            idx_selected = jax.random.choice(key, idx, (N, ), replace=False, p=p_arr)
            error = error_merged[idx_selected]

            leaves_selected = [leaves_merged[i][idx_selected] for i in range(len(leaves_parent))]
            population = jax.tree.unflatten(treedef, leaves_selected)
            
        else:
            raise NotImplementedError(f"Only greedy and global are available as selection_mechanism. Not {selection_mechanism}")
        
        return population, error
    
    



    def step(self, descent_state, measurement_info, descent_info):
        """ 
        Performs one step/generation of the Differential Evolution Algorithm. 

        Args:
            descent_state: Pytree, 
            measurement_info: Pytree,
            descent_info: Pytree,

        Returns:
            tuple[Pytree, jnp.array], the updated descent state, the mean, minimum and maximum error of the current population

        """

        mutation_strategy, mutation_rate, crossover_strategy, crossover_rate = descent_info.mutation_strategy, descent_info.mutation_rate, descent_info.crossover_strategy, descent_info.crossover_rate
        selection_mechanism = descent_info.selection_mechanism
        best_individual, parent_population, key = descent_state.best_individual, descent_state.population, descent_state.key

        key, subkey = jax.random.split(key, 2)
        descent_state = tree_at(lambda x: x.key, descent_state, key)
        key1, key2, key3 = jax.random.split(subkey, 3)

        mutant_population = self.do_mutation(mutation_strategy, mutation_rate, best_individual, parent_population, key1)
        trial_population = self.do_crossover(crossover_strategy, crossover_rate, parent_population, mutant_population, key2)

        trace_error_parents = self.calculate_error_population(parent_population, measurement_info, descent_info)
        trace_error_trial = self.calculate_error_population(trial_population, measurement_info, descent_info)

        population, error = self.select_population(key3, selection_mechanism, trace_error_parents, trace_error_trial, parent_population, trial_population, descent_info)

        descent_state = tree_at(lambda x: x.population, descent_state, population)
        error_mean = jnp.mean(error)
        error_min = jnp.min(error)
        error_max = jnp.max(error)

        descent_state = tree_at(lambda x: x.best_individual, descent_state, self.get_individual_from_idx(jnp.argmin(error), population))
        return descent_state, jnp.array([error_mean, error_min, error_max])
    


    def initialize_run(self, population):
        """
        Prepares all provided data and parameters for the reconstruction. 
        Here the final shape/structure of descent_state, measurement_info and descent_info are determined. 

        Args:
            population: Pytree, the initial guess as created by self.create_initial_population()
        
        Returns:
            tuple[Pytree, Callable], the initial descent state, the step-function of the algorithm.

        """

        self.initialize_general_optimizer(population)
        measurement_info = self.measurement_info

        mutation_strategy, crossover_strategy  = self.strategy.split("_")
        self.descent_info = self.descent_info.expand(mutation_strategy = mutation_strategy, 
                                                     crossover_strategy = crossover_strategy,
                                                     mutation_rate = self.mutation_rate,
                                                     crossover_rate = self.crossover_rate,
                                                     selection_mechanism = self.selection_mechanism,
                                                     temperature = self.temperature)
        descent_info = self.descent_info
        
        trace_error = self.calculate_error_population(population, measurement_info, descent_info)
        self.descent_state = self.descent_state.expand(best_individual = self.get_individual_from_idx(jnp.argmin(trace_error), population), 
                                                       key=self.key)
        descent_state = self.descent_state
        
        do_step = Partial(self.step, measurement_info=measurement_info, descent_info=descent_info)
        do_step = Partial(scan_helper, actual_function=do_step, number_of_args=1, number_of_xs=0)
        return descent_state, do_step













class EvosaxBASE(GeneralOptimizationBASE):
    """
    Employs the evosax package to perform the optimization. Inherits from GeneralOptimizationBASE.

    Attributes:
        solver: evosax-solver,

    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        self.name = "Evosax"
        self.solver = None
        


    def step_amp_or_phase(self, population_amp_or_phase, descent_state, measurement_info, descent_info, amp_or_phase):
        """ 
        Evosax does not respect the structure of a pytree. Thus when optimizing parameters with vastly different properties simultaneously
        one should expose them seperately to evosax in order to avoid mixing. Thus the step function is applied once to update the amplitudes 
        and once to update the phases.
        """

        solver = getattr(descent_info.solver, amp_or_phase)
        state = getattr(descent_state.evosax.state, amp_or_phase)
        params = getattr(descent_state.evosax.params, amp_or_phase)
        key = descent_state.key

        key, subkey = jax.random.split(key)
        key_ask, key_tell, key_ask_2 = jax.random.split(subkey, 3)

        population, state = solver.ask(key_ask, state, params)

        if amp_or_phase=="amp":
            population_amp, population_phase = population, population_amp_or_phase
        elif amp_or_phase=="phase":
            population_amp, population_phase = population_amp_or_phase, population

        population_eval = self.merge_population_from_amp_and_phase(population_amp, population_phase)

        fitness = self.calculate_error_population(population_eval, measurement_info, descent_info)
        state, metrics = solver.tell(key_tell, population, fitness, state, params)
        population, state = solver.ask(key_ask_2, state, params)

        descent_state = tree_at(lambda x: getattr(x.evosax.state, amp_or_phase), descent_state, state)
        descent_state = tree_at(lambda x: x.key, descent_state, key)
        return descent_state, population, fitness


        
    def step(self, descent_state, measurement_info, descent_info):
        """
        Performs one optimization step through an evolutionary algorithm in evosax.

        Args:
            descent_state: Pytree, 
            measurement_info: Pytree,
            descent_info: Pytree,

        Returns:
            tuple[Pytree, jnp.array], the updated descent state, the mean, minimum and maximum error of the current population

        """

        population_amp, population_phase = self.split_population_in_amp_and_phase(descent_state.population)

        if descent_info.measured_spectrum_is_provided.pulse==False or descent_info.measured_spectrum_is_provided.gate==False:
            descent_state, population_amp, fitness = self.step_amp_or_phase(population_phase, descent_state, measurement_info, descent_info, "amp")

        descent_state, population_phase, fitness = self.step_amp_or_phase(population_amp, descent_state, measurement_info, descent_info, "phase")

        population = self.merge_population_from_amp_and_phase(population_amp, population_phase)
        descent_state = tree_at(lambda x: x.population, descent_state, population)

        errors = jnp.array([jnp.mean(fitness), jnp.min(fitness), jnp.max(fitness)])
        return descent_state, errors
    



    def initialize_evosax_solver(self, key, solver, population, individual, amp_or_phase):
        """ 
        Initializes the provided evosax solver. 
        The solvers need to be exposed to the entire population, one individual or the initial fitnesses.
        Since the amplitude and phase are optimized separetly a solver needs to be initialized once for each. (This done by calling this method twice.)
        """

        key, subkey = jax.random.split(key, 2)

        params = solver.default_params
        class_str = str(self.solver)
        if class_str.split(".")[2]=="population_based":
            fitness = self.calculate_error_population(population, self.measurement_info, self.descent_info)

            population_amp, population_phase = self.split_population_in_amp_and_phase(population)

            if amp_or_phase=="amp":
                population = population_amp
            elif amp_or_phase=="phase":
                population = population_phase
            else:
                raise ValueError(f"amp_or_phase needs to be amp or phase. Not {amp_or_phase}")

            state = solver.init(subkey, population, fitness, params)


        elif class_str.split(".")[2]=="distribution_based":
            state = solver.init(subkey, individual, params)

        else:
            raise ValueError(f"Something went wrong in the detection of evosax's submodule. Needs to be \
                             population_based or distribution_based. Found {class_str}")

        return state, params, key



    def initialize_run(self, population):
        """
        Prepares all provided data and parameters for the reconstruction. Initializes the evosax-solver appropriately.
        Here the final shape/structure of descent_state, measurement_info and descent_info are determined. 

        Args:
            population: Pytree, the initial guess as created by self.create_initial_population()
        
        Returns:
            tuple[Pytree, Callable], the initial descent state, the step-function of the algorithm.

        """

        self.initialize_general_optimizer(population)
        measurement_info=self.measurement_info

        population_size = self.descent_info.population_size
        individual = self.get_individual_from_idx(0, population)
        individual_amp, individual_phase = self.split_population_in_amp_and_phase(individual)

        if type(self.solver)!=tuple or type(self.solver)!=list:
            solver_amp = self.solver
            solver_phase = self.solver
        elif len(self.solver)==2:
            solver_amp, solver_phase = self.solver
        else:
            raise ValueError(f"solver needs to be an evosax class or a list/tuple of two of its solvers. Got {self.solver}.")

        self.descent_info = self.descent_info.expand(solver = MyNamespace(amp=solver_amp(population_size=population_size, solution=individual_amp), 
                                                                          phase=solver_phase(population_size=population_size, solution=individual_phase)))
        descent_info=self.descent_info

        if descent_info.measured_spectrum_is_provided.pulse==False or descent_info.measured_spectrum_is_provided.gate==False:
            state_amp, params_amp, self.key = self.initialize_evosax_solver(self.key, descent_info.solver.amp, population, individual_amp, "amp")
        else:
            state_amp, params_amp = None, None

        state_phase, params_phase, self.key = self.initialize_evosax_solver(self.key, descent_info.solver.phase, population, individual_phase, "phase")
        self.descent_state = self.descent_state.expand(evosax=MyNamespace(state=MyNamespace(amp=state_amp, phase=state_phase), 
                                                                          params=MyNamespace(amp=params_amp, phase=params_phase)), 
                                                       key = self.key)
        descent_state = self.descent_state
        
        do_step=Partial(self.step, measurement_info=measurement_info, descent_info=descent_info)
        do_step=Partial(scan_helper, actual_function=do_step, number_of_args=1, number_of_xs=0)
        return descent_state, do_step














class LSFBASE(GeneralOptimizationBASE):
    # maybe there is an issue here. 
    # In one iteration all bisection steps are done first on the pulse and only then on the gate. Maybe this should be alternating?

    """
    Implements a version of the Linesearch FROG Algorithm (LSF). Despite its name the algorithm is NOT restricted to FROG. 
    Inherits from GeneralOptimizationBASE.

    The algorithm is not implemented for the optimization of parametrized populations. Instead a population is always evaluated on the time/frequency axis
    and thus optimized in its discretized form.

    Attributes:
        number_of_bisection_iterations: int,
        random_direction_mode: str,
        no_points_for_continuous: int,
        boundary: int,

    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.name = "LSF"

        self.number_of_bisection_iterations = 12

        self.random_direction_mode = "random"
        self.no_points_for_continuous = 5

        self.boundary = 1 

    

    def get_random_values(self, key, shape, minval, maxval, descent_info):
        """ LSF requires random directions. These are produced here. """
        mode = descent_info.random_direction_mode
        no_points_for_continuous = descent_info.no_points_for_continuous

        if mode=="random":
            values = jax.random.uniform(key, shape, minval=minval, maxval=maxval)

        elif mode=="continuous":
            x_new = jnp.linspace(0, 1, shape[0])
            N = shape[0]//no_points_for_continuous

            key1, key2 = jax.random.split(key, 2)
            x = jnp.sort(jax.random.choice(key1, x_new, (N, ), replace=False))
            points = jax.random.uniform(key2, (N, ), minval=minval, maxval=maxval)

            values = do_interpolation_1d(x_new, x, points)
            values = values/jnp.max(jnp.abs(values))*jnp.maximum(jnp.abs(minval), jnp.abs(maxval))
        else:
            raise NotImplementedError(f"random_direction_mode needs to be random or continuous. Not {mode}")

        return values
    


    def get_search_direction_individual(self, keys, individual, measurement_info, descent_info):
        """ Creates a pytree with random search directions for one individual. """
        key1, key2 = keys.pulse
        direction = MyNamespace(pulse=None, gate=None)

        pulse = individual.pulse
        shape_pulse = jnp.shape(pulse)
        d_pulse_re = self.get_random_values(key1, shape_pulse, -1, 1, descent_info)
        d_pulse_im = self.get_random_values(key2, shape_pulse, -1, 1, descent_info)
        d = d_pulse_re + 1j*d_pulse_im
        direction_pulse = d/jnp.linalg.norm(d)
        direction = tree_at(lambda x: x.pulse, direction, direction_pulse, is_leaf=lambda x: x is None)

        if measurement_info.doubleblind==True:
            key3, key4 = keys.gate

            gate = individual.gate
            shape_gate = jnp.shape(gate)
            d_gate_re = self.get_random_values(key3, shape_gate, -1, 1, descent_info)
            d_gate_im = self.get_random_values(key4, shape_gate, -1, 1, descent_info)
            d = d_gate_re + 1j*d_gate_im
            direction_gate = d/jnp.linalg.norm(d)
            direction = tree_at(lambda x: x.gate, direction, direction_gate, is_leaf=lambda x: x is None)

        return direction



    def get_search_direction(self, key, population, measurement_info, descent_info):
        """ Creates a pytree with random search directions for an entire population. """
        leaves, treedef = jax.tree.flatten(population)
        keys = jax.random.split(key, len(leaves))
        keys = [jax.random.split(keys[i], jnp.shape(leaves[i])[0]*2).reshape(jnp.shape(leaves[i])[0], 2, 2) for i in range(len(leaves))]
        key_tree = jax.tree.unflatten(treedef, keys)
        return jax.vmap(self.get_search_direction_individual, in_axes=(0, 0, None, None))(key_tree, population, measurement_info, descent_info)




    def get_scalars(self, direction, signal, descent_info):
        """ Calculates scalars to identify the min/max of a search direction. """
        # solve jnp.abs(signal + s*direction)**2 = jnp.abs(boundary)**2
        p = 2*jnp.real(signal*jnp.conjugate(direction))/(jnp.abs(direction)**2+1e-9)
        q = (jnp.abs(signal)**2 - jnp.abs(descent_info.boundary)**2)/(jnp.abs(direction)**2+1e-9)

        diskriminante = p**2/4 - q
        diskriminante = jnp.maximum(diskriminante, 0)
        s1 = -p/2 - jnp.sqrt(diskriminante)
        s2 = -p/2 + jnp.sqrt(diskriminante)
        return jnp.max(s1, axis=1)[:, jnp.newaxis], jnp.min(s2, axis=1)[:, jnp.newaxis]




    def bisection_step_logic_0(self, El, Em, Er, signal):
        return Em, Er

    def bisection_step_logic_1(self, El, Em, Er, signal):
        dl = jnp.linalg.norm(signal - El)
        dr = jnp.linalg.norm(signal - Er)

        x=jnp.sign(dr-dl)
        condition=(x+1)//2
        El = El*condition + Em*(1-condition)
        Er = Em*condition + Er*(1-condition)
        return El, Er

    def bisection_step_logic_2(self, El, Em, Er, signal):
        return El, Em
    

    def bisection_step(self, El, Er, population, measurement_info, descent_info, pulse_or_gate):
        """ Does one bisection step of the LSF algorithm. """

        Em = (El + Er)/2
        E_arr=jnp.array([El, Em, Er])
        
        error_arr = jax.vmap(self.calculate_error, in_axes=(0, None, None, None, None))(E_arr, population, measurement_info, descent_info, pulse_or_gate)
        idx = jnp.argmax(error_arr, axis=0)

        El, Er = jax.vmap(jax.lax.switch, in_axes=(0, None, 0, 0, 0, None))(idx, [self.bisection_step_logic_0, 
                                                                                  self.bisection_step_logic_1, 
                                                                                  self.bisection_step_logic_2], El, Em, Er, getattr(population, pulse_or_gate))
        return (El, Er), None
    



    def do_bisection_search(self, direction, population, measurement_info, descent_info, pulse_or_gate):
        """ 
        Performs one bisection search to find the minimum along a given search direction. 
        The number of iterations is set through self.number_of_bisection_iterations
        """
        s1, s2 = self.get_scalars(direction, getattr(population, pulse_or_gate), descent_info)

        El = getattr(population, pulse_or_gate) + s1*direction
        Er = getattr(population, pulse_or_gate) + s2*direction
        
        no_iterations = descent_info.number_of_bisection_iterations
        do_bisection_step = Partial(self.bisection_step, population=population, measurement_info=measurement_info, 
                                    descent_info=descent_info, pulse_or_gate=pulse_or_gate)
        
        do_step = Partial(scan_helper, actual_function=do_bisection_step, number_of_args=2, number_of_xs=0)
        E_arr, _ = jax.lax.scan(do_step, (El, Er), length=no_iterations) 
        E_arr = jnp.array(E_arr)

        error_arr = jax.vmap(self.calculate_error, in_axes=(0, None, None, None, None))(E_arr, population, measurement_info, descent_info, pulse_or_gate)
        idx = jnp.argmin(error_arr, axis=0)
        return jax.vmap(jax.lax.switch, in_axes=(0, None, 0))(idx, [lambda x: x[0], lambda x: x[1]], jnp.transpose(E_arr, axes=(1,0,2)))




    def search_along_direction(self, direction, population, measurement_info, descent_info):
        """ Performs a bisection search along one direction for pulse and the for gate. """
        direction_pulse = direction.pulse
        population_pulse = self.do_bisection_search(direction_pulse, population, measurement_info, descent_info, "pulse")
        population = tree_at(lambda x: x.pulse, population, population_pulse)
        
        if measurement_info.doubleblind==True:
            direction_gate=direction.gate
            population_gate = self.do_bisection_search(direction_gate, population, measurement_info, descent_info, "gate")
            population = tree_at(lambda x: x.gate, population, population_gate)

        return population
    
    


    def calculate_error(self, E_arr, population, measurement_info, descent_info, pulse_or_gate):
        """ 
        Calculates the trace error. Since pulse and gate are optimized independently the population as provided to calculate_error_individual() 
        needs to be constructed from the current population and the fields in the cureent optimization.
        """

        if pulse_or_gate=="pulse":
            pulse_arr, gate_arr = E_arr, population.gate

        elif pulse_or_gate=="gate":
            pulse_arr, gate_arr = population.pulse, E_arr
        else:
            raise ValueError(f"pulse_or_gate needs to be pulse or gate. Not {pulse_or_gate}")

        error_arr = jax.vmap(self.calculate_error_individual, 
                             in_axes=(0, None, None))(MyNamespace(pulse=pulse_arr, gate=gate_arr), measurement_info, descent_info)
        return error_arr
    




    def step(self, descent_state, measurement_info, descent_info):
        """
        Performs one step of the LSF-algorithm.

        Args:
            descent_state: Pytree, 
            measurement_info: Pytree,
            descent_info: Pytree,

        Returns:
            tuple[Pytree, jnp.array], the updated descent state, errors of the current population

        """

        population = descent_state.population
        key, subkey = jax.random.split(descent_state.key, 2)
        descent_state = tree_at(lambda x: x.key, descent_state, key)
        
        direction = self.get_search_direction(subkey, population, measurement_info, descent_info)
        population = self.search_along_direction(direction, population, measurement_info, descent_info)

        error_arr = self.calculate_error_population(population, measurement_info, descent_info)
        descent_state = tree_at(lambda x: x.population, descent_state, population)
        return descent_state, error_arr.reshape(-1,1)
    



    def initialize_run(self, population):
        """
        Prepares all provided data and parameters for the reconstruction.
        Here the final shape/structure of descent_state, measurement_info and descent_info are determined. 

        Args:
            population: Pytree, the initial guess as created by self.create_initial_population()
        
        Returns:
            tuple[Pytree, Callable], the initial descent state, the step-function of the algorithm.

        """

        if self.descent_info.amp_type!="discrete" or self.descent_info.phase_type!="discrete":
            print("LSF is only implemented for amp_type=discrete, phase_type=discrete and converts the populations accordingly.")

        population = self.convert_population(population, self.measurement_info, self.descent_info)
        population_pulse = jax.vmap(lambda x: x/jnp.linalg.norm(x))(population.pulse)
        population = tree_at(lambda x: x.pulse, population, population_pulse)
        self.initialize_general_optimizer(population)

        measurement_info = self.measurement_info

        self.descent_info = self.descent_info.expand(number_of_bisection_iterations = self.number_of_bisection_iterations,
                                                     no_points_for_continuous = self.no_points_for_continuous,
                                                     random_direction_mode = self.random_direction_mode,
                                                     boundary = self.boundary)
        descent_info = self.descent_info

        self.descent_state = self.descent_state.expand(key = self.key)
        descent_state = self.descent_state

        do_step = Partial(self.step, measurement_info=measurement_info, descent_info=descent_info)
        do_step = Partial(scan_helper, actual_function=do_step, number_of_args=1, number_of_xs=0)
        return descent_state, do_step





    def post_process_get_pulse_and_gate(self, descent_state, measurement_info, descent_info):
        """ Some custom post-processing since the optimization is done in the explicit deiscretized form. """
        sk, rn = measurement_info.sk, measurement_info.rn
        idx = self.get_idx_best_individual(descent_state)

        individual = self.get_individual_from_idx(idx, descent_state.population)

        pulse_t = individual.pulse
        gate_t = individual.gate
        pulse_f = do_fft(pulse_t, sk, rn) 
        gate_f = do_fft(gate_t, sk, rn) 
        return pulse_t, gate_t, pulse_f, gate_f
    
















class AutoDiffBASE(GeneralOptimizationBASE):
    """
    Employs the optimistix package to perform the optimization via Automatic-Differentiation. Inherits from GeneralOptimizationBASE.
    Is not implemented to optimize over a population. Instead only one individual is optimized.

    Attributes:
        solver: optimistix-solver,
        alternating_optimization: bool,
        optimize_individual_idx: int,
    
    """

    # Making this work for a population: (?) 
    #    vmap over solver.init and over solver.step 
    #          -> currently only equinox.filter_vmap for solver.init 
    #          -> doesnt vmap some things 
    #          -> vmap over solver.step doesnt work becasue of wrong leaf dimensions
    #   
    #    repeatedly calling optimistix.minimise does not work because of limited recursion depth for jax. 

    #        print("using jax.scipy minimize might work with vmap") -> jax.scipy is not really developed. only a prelimiary version of lbfgs

    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.name = "AutoDiff"

        self.solver = optimistix.BFGS
        self.alternating_optimization = False

        self.optimize_individual_idx = 0


    def get_phase(self, coefficients, central_f, measurement_info, descent_info):
        """ Wraps around GeneralOptimizationBASE.get_phase() in order to fascilliate the optimization of the group delay. """
        phase = super().get_phase(coefficients, central_f, measurement_info, descent_info)
        if descent_info.phase_type=="polynomial" or descent_info.phase_type=="sinusoidal":
            return phase
        else:
            return jnp.cumsum(phase)*measurement_info.df



    def loss_function(self, individual, measurement_info, descent_info):
        """ Wraps around self.calculate_error_individual() to return the error of the current guess. """
        pulse = self.make_pulse_from_individual(individual, measurement_info, descent_info, "pulse")

        if measurement_info.doubleblind==True:
            gate = self.make_pulse_from_individual(individual, measurement_info, descent_info, "gate")
        else:
            gate = pulse
            
        trace_error = self.calculate_error_individual(MyNamespace(pulse=pulse, gate=gate), measurement_info, descent_info)
        return trace_error
    


    def loss_function_amp(self, individual_amp, individual_phase, measurement_info, descent_info):
        """ Helper to fascilliate optimization with respect to the amplitude only. """
        individual = self.merge_population_from_amp_and_phase(individual_amp, individual_phase)
        trace_error = self.loss_function(individual, measurement_info, descent_info)
        return trace_error
    

    def loss_function_phase(self, individual_phase, individual_amp, measurement_info, descent_info):
        """ Helper to fascilliate optimization with respect to the phase only. """
        individual = self.merge_population_from_amp_and_phase(individual_amp, individual_phase)
        trace_error = self.loss_function(individual, measurement_info, descent_info)
        return trace_error
    



    def step(self, descent_state, measurement_info, descent_info):
        """
        Performs one optimization step.

        Args:
            descent_state: Pytree, 
            measurement_info: Pytree,
            descent_info: Pytree,

        Returns:
            tuple[Pytree, jnp.array], the updated descent state, errors of the current population

        """

        if descent_info.alternating_optimization==False:
            individual, optimistix_state = descent_state.individual, descent_state.optimistix_state
            individual, optimistix_state, error = self.optimistix_step(y=individual, state=optimistix_state)

            descent_state = tree_at(lambda x: x.individual, descent_state, individual)
            descent_state = tree_at(lambda x: x.optimistix_state, descent_state, optimistix_state)

        elif descent_info.alternating_optimization==True:
            individual, optimistix_state_amp, optimistix_state_phase = descent_state.individual, descent_state.optimistix_state_amp, descent_state.optimistix_state_phase

            individual_amp, individual_phase = self.split_population_in_amp_and_phase(individual)
            
            individual_amp, optimistix_state_amp, error_amp = self.optimistix_step_amp(y=individual_amp, args=individual_phase, state=optimistix_state_amp)
            individual_phase, optimistix_state_phase, error = self.optimistix_step_phase(y=individual_phase, args=individual_amp, state=optimistix_state_phase)
            
            individual = self.merge_population_from_amp_and_phase(individual_amp, individual_phase)

            descent_state = tree_at(lambda x: x.individual, descent_state, individual)
            descent_state = tree_at(lambda x: x.optimistix_state_amp, descent_state, optimistix_state_amp)
            descent_state = tree_at(lambda x: x.optimistix_state_phase, descent_state, optimistix_state_phase)
            
        else:
            raise ValueError(f"alternating_optimization needs to be True/False. Not {descent_info.alternating_optimization}")

        return descent_state, error
    
    


    def initialize_alternating_optimization(self, descent_state, individual, solver, optimistix_args, measurement_info, descent_info):
        """ If the amplitude and phase are supposed to be optimized in an alternating fashion instead of simultaneously, different methods have to be initialized. """
        options, f_struct, aux_struct, tags = optimistix_args

        individual_amp, individual_phase = self.split_population_in_amp_and_phase(individual)

        loss_function_amp = Partial(self.loss_function_amp, measurement_info=measurement_info, descent_info=descent_info)
        loss_function_amp = Partial(optimistix_helper_loss_function, function=loss_function_amp, no_of_args=1)
        
        loss_function_phase = Partial(self.loss_function_phase, measurement_info=measurement_info, descent_info=descent_info)
        loss_function_phase = Partial(optimistix_helper_loss_function, function=loss_function_phase, no_of_args=1)


        state_amp = solver.init(loss_function_amp, individual_amp, individual_phase, options, f_struct, aux_struct, tags)
        state_phase = solver.init(loss_function_phase, individual_phase, individual_amp, options, f_struct, aux_struct, tags)
        
        self.optimistix_step_amp = Partial(solver.step, fn=loss_function_amp, options=options, tags=tags)
        self.optimistix_step_phase = Partial(solver.step, fn=loss_function_phase, options=options, tags=tags)

        descent_state = descent_state.expand(optimistix_state_amp = state_amp,
                                             optimistix_state_phase = state_phase)
        return descent_state
        


    def initialize_optimistix(self, descent_state, solver, measurement_info, descent_info):
        """
        Initializes the optimistix-solver.
        """
        
        if solver.__class__.__module__.split(".")[0]=="optax":
            solver=optimistix.OptaxMinimiser(solver, rtol=1, atol=1)
        else:
            solver=solver(rtol=1, atol=1)

        loss_function = Partial(self.loss_function, measurement_info=measurement_info, descent_info=descent_info)
        loss_function = Partial(optimistix_helper_loss_function, function=loss_function, no_of_args=0)

        args = None
        options = dict()
        f_struct = jax.ShapeDtypeStruct((), jnp.float32)
        aux_struct = jax.ShapeDtypeStruct((), jnp.float32)
        tags = frozenset()

        individual = descent_state.individual
        if descent_info.alternating_optimization==False:
            state = solver.init(loss_function, individual, args, options, f_struct, aux_struct, tags)
            self.optimistix_step = Partial(solver.step, args=args, fn=loss_function, options=options, tags=tags)
            descent_state = descent_state.expand(optimistix_state = state)

        elif descent_info.alternating_optimization==True:
            optimistix_args = (options, f_struct, aux_struct, tags)
            descent_state = self.initialize_alternating_optimization(descent_state, individual, solver, optimistix_args, measurement_info, descent_info)

        else:
            raise ValueError(f"alternating_optimization needs to be True/False. Not {descent_info.alternating_optimization}")
            

        descent_state, static = equinox.partition(descent_state, equinox.is_array)

        do_step = Partial(self.step, measurement_info=measurement_info, descent_info=descent_info)
        do_step = Partial(scan_helper_equinox, step=do_step, static=static)
        return descent_state, do_step

    

    
    def initialize_run(self, population):
        """
        Prepares all provided data and parameters for the reconstruction.
        Here the final shape/structure of descent_state, measurement_info and descent_info are determined. 

        Args:
            population: Pytree, the initial guess as created by self.create_initial_population()
        
        Returns:
            tuple[Pytree, Callable], the initial descent state, the step-function of the algorithm.

        """

        if self.descent_info.population_size!=1:
            print("AD will only optimize one individual in the population. This is because there are some issues when using vmap.")
            print("You can select the individual to be optimized via self.optimize_individual_idx")
            
        self.initialize_general_optimizer(population)

        measurement_info = self.measurement_info

        self.descent_info = self.descent_info.expand(alternating_optimization=self.alternating_optimization)
        descent_info = self.descent_info

        self.descent_state = self.descent_state.expand(individual = self.get_individual_from_idx(self.optimize_individual_idx, population))
        descent_state = self.descent_state

        descent_state, do_scan = self.initialize_optimistix(descent_state, self.solver, measurement_info, descent_info)
        return descent_state, do_scan
    

    

    
